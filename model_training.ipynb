{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at e:\\swe\\python3.11.5\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "pip -q install --upgrade ruptures scikit-learn tqdm matplotlib joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from datetime import date, datetime, timedelta\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "import ruptures as rpt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Folders for Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'AffectiveROAD_Dataset/AffectiveROAD_Data/Database/'\n",
    "features_path = 'E4/'\n",
    "label_path = 'Subj_metric'\n",
    "processed_dataset_path = 'Processed_Dataset/'\n",
    "model_path = 'Models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(processed_dataset_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Features Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features_folders(base_directory, processed_dataset_path):\n",
    "    \n",
    "    # Loop through serial numbers from 1 to 13\n",
    "    for serial_number in tqdm(range(1, 14), desc=\"Processing folders\"):\n",
    "        folder_name = f'{serial_number}-E4-Drv{serial_number}'\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "\n",
    "        # Check if the folder exists\n",
    "        if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "            \n",
    "            # List files in the folder\n",
    "            files_in_folder = os.listdir(folder_path)\n",
    "\n",
    "            # Unzip each zip file in the folder\n",
    "            for file_name in tqdm(files_in_folder, desc=\"Unzipping files\", leave=False):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if file_name.endswith('.zip'):\n",
    "                    \n",
    "                    # Create a new folder for each unzipped content\n",
    "                    new_folder_name = f'{serial_number}_unzipped'\n",
    "                    new_folder_path = os.path.join(processed_dataset_path, new_folder_name)\n",
    "                    os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "                    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(new_folder_path)\n",
    "\n",
    "        else:\n",
    "            tqdm.write(f\"Folder {folder_name} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 13/13 [00:00<00:00, 18.48it/s]\n"
     ]
    }
   ],
   "source": [
    "process_features_folders((dataset_path + features_path), processed_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Label Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label_files(base_directory, processed_dataset_path):\n",
    "    \n",
    "    # Loop through serial numbers from 1 to 13\n",
    "    for serial_number in tqdm(range(1, 14), desc=\"Processing files\"):\n",
    "        file_name = f'SM_Drv{serial_number}.csv'\n",
    "        file_path = os.path.join(base_directory, file_name)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "\n",
    "            # New filepath to copy file to\n",
    "            new_folder_name = f'{serial_number}_unzipped'\n",
    "            new_folder_path = os.path.join(processed_dataset_path, new_folder_name)\n",
    "\n",
    "            # Copy the file to the new folder\n",
    "            new_file_path = os.path.join(new_folder_path, file_name)\n",
    "            shutil.copy(file_path, new_file_path)\n",
    "\n",
    "        else:\n",
    "            tqdm.write(f\"File {file_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 13/13 [00:00<00:00, 1444.40it/s]\n"
     ]
    }
   ],
   "source": [
    "process_label_files((dataset_path + label_path), processed_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Processed Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HR Data Processing\n",
    "\n",
    "- HR data relatively less than EDA and TEMP so data is repeated 4 times to have similar amount of data as the other readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hr_data(base_path):\n",
    "\n",
    "    # Loop through folders with names in the format X_unzipped\n",
    "    for serial_number in tqdm(range(1, 14), desc=\"Processing HR Data\"):\n",
    "        folder_name = f\"{serial_number}_unzipped\"\n",
    "        current_folder_path = os.path.join(base_path, folder_name)\n",
    "\n",
    "        # Check if the folder exists\n",
    "        if os.path.exists(current_folder_path) and os.path.isdir(current_folder_path):\n",
    "            # Define the path to the HR.csv file in the current folder\n",
    "            hr_data_path = os.path.join(current_folder_path, \"HR.csv\")\n",
    "\n",
    "            try:\n",
    "                # Load, repeat, and save the HR data to HR_new.csv\n",
    "                hr_data = np.loadtxt(hr_data_path, delimiter=',')\n",
    "                hr_data = np.repeat(hr_data, 4)\n",
    "                np.savetxt(os.path.join(current_folder_path, \"HR_new.csv\"), hr_data, delimiter=',')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing HR data in {folder_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing HR Data: 100%|██████████| 13/13 [00:00<00:00, 15.89it/s]\n"
     ]
    }
   ],
   "source": [
    "process_hr_data(processed_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_one_folder(serial_number, base_path):\n",
    "    \n",
    "    folder_name = f\"{serial_number}_unzipped\"\n",
    "    current_folder_path = os.path.join(base_path, folder_name)\n",
    "    \n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(current_folder_path) and os.path.isdir(current_folder_path):\n",
    "        # Define file paths for EDA, HR_new, TEMP, and STRESS data\n",
    "        eda_path = os.path.join(current_folder_path, \"EDA.csv\")\n",
    "        hr_path = os.path.join(current_folder_path, \"HR_new.csv\")\n",
    "        temp_path = os.path.join(current_folder_path, \"TEMP.csv\")\n",
    "        stress_path = os.path.join(current_folder_path, f\"SM_Drv{serial_number}.csv\")\n",
    "\n",
    "        # Read data frames\n",
    "        eda = pd.read_csv(eda_path, header=2, names=['EDA'])\n",
    "        hr = pd.read_csv(hr_path, header=12, names=['HR'])\n",
    "        temp = pd.read_csv(temp_path, header=2, names=['TEMP'])\n",
    "        stress = pd.read_csv(stress_path, header=1, names=['STRESS'])\n",
    "\n",
    "        # Determine the minimum length among data frames\n",
    "        min_len = min(len(eda), len(hr), len(temp), len(stress))\n",
    "\n",
    "        # Take the first min_len rows from each data frame\n",
    "        eda = eda.iloc[:min_len, 0]\n",
    "        hr = hr.iloc[:min_len, 0]\n",
    "        temp = temp.iloc[:min_len, 0]\n",
    "        stress = stress.iloc[:min_len, 0]\n",
    "        \n",
    "        # Concatenate the data column-wise\n",
    "        df_original = pd.concat([eda, hr, temp, stress], axis = 1)\n",
    "    \n",
    "        return df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_csv(base_path):\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    df_combined = pd.DataFrame()\n",
    "\n",
    "    # Loop through folders with names in the format X_unzipped\n",
    "    for serial_number in tqdm(range(1, 14), desc=\"Processing Folders\"):\n",
    "        \n",
    "        df_original = dataframe_one_folder(serial_number, base_path)\n",
    "        df_combined = pd.concat([df_combined, df_original], ignore_index=True)\n",
    "\n",
    "    df_combined.to_csv(f'{base_path}df_combined_data.csv', index=False)\n",
    "    \n",
    "    print(\"CSV File saved successfully: \", df_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Folders: 100%|██████████| 13/13 [00:00<00:00, 46.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File saved successfully:  (194555, 4)\n"
     ]
    }
   ],
   "source": [
    "combined_csv(processed_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_features(arr):\n",
    "    vmin = np.amin(arr)\n",
    "    vmax = np.amax(arr)\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    return vmin, vmax, mean, std\n",
    "\n",
    "def shape_features(arr):\n",
    "    skewness = skew(arr)\n",
    "    kurt = kurtosis(arr)\n",
    "    return skewness, kurt\n",
    "\n",
    "def calculate_rms(signal):\n",
    "    diff_squared = np.square(np.ediff1d(signal))\n",
    "    rms_value = np.sqrt(np.mean(diff_squared))\n",
    "    return rms_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Features\n",
    "\n",
    "- iterating with a step size of 20\n",
    "- taking 40 rows at a time to generate a single row of df_features\n",
    "- find_peaks() : identify peaks in the EDA signal (eda) using the function and then count the number of detected peaks using len() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    cols = [\n",
    "        'EDA_Mean', 'EDA_Min', 'EDA_Max', 'EDA_Std', 'EDA_Kurtosis', 'EDA_Skew', 'EDA_Num_Peaks', 'EDA_Amplitude', 'EDA_Duration',\n",
    "        'HR_Mean', 'HR_Min', 'HR_Max', 'HR_Std', 'HR_RMS', 'TEMP_Mean', 'TEMP_Min', 'TEMP_Max', 'TEMP_Std', 'STRESS'\n",
    "    ]\n",
    "\n",
    "    df_features = pd.DataFrame(columns=cols)\n",
    "    index = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(data['EDA']), 20), desc=\"Processing rows\", leave=True):\n",
    "        df_partial = data.iloc[i:i+40,]\n",
    "        plen = len(df_partial['EDA'])\n",
    "\n",
    "        if plen < 40:\n",
    "            continue\n",
    "\n",
    "        eda = df_partial['EDA'].values\n",
    "        hr = df_partial['HR'].values\n",
    "        temp = df_partial['TEMP'].values\n",
    "        stress = df_partial['STRESS'].values\n",
    "\n",
    "        eda_min, eda_max, eda_mean, eda_std = statistical_features(eda)\n",
    "        hr_min, hr_max, hr_mean, hr_std = statistical_features(hr)\n",
    "        temp_min, temp_max, temp_mean, temp_std = statistical_features(temp)\n",
    "        stress_min, stress_max, stress_mean, stress_std = statistical_features(stress)\n",
    "        eda_skew, eda_kurtosis = shape_features(eda)\n",
    "\n",
    "        hr_rms = calculate_rms(hr)\n",
    "        temp_rms = calculate_rms(temp)\n",
    "\n",
    "        peaks, properties = find_peaks(eda, width=5)\n",
    "        num_Peaks = len(peaks)\n",
    "\n",
    "        prominences = np.array(properties['prominences'])\n",
    "        widths = np.array(properties['widths'])\n",
    "        amplitude = np.sum(prominences)\n",
    "        duration = np.sum(widths)\n",
    "\n",
    "        df_features.loc[index] = [eda_mean, eda_min, eda_max, eda_std, eda_kurtosis, eda_skew, num_Peaks, amplitude,\n",
    "                                  duration, hr_mean, hr_min, hr_max, hr_std, hr_rms, temp_mean, temp_min, temp_max, temp_std, stress_mean]\n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    " df_combined = pd.read_csv(f'{processed_dataset_path}df_combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 9728/9728 [00:14<00:00, 687.35it/s]\n"
     ]
    }
   ],
   "source": [
    "df_features = extract_features(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9726, 19)\n"
     ]
    }
   ],
   "source": [
    "print(df_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lag_features(input_df, columns, lags):\n",
    "    cols = list(map(str, range(len(columns) * len(lags), 0, -1)))\n",
    "    lag_df = pd.DataFrame(columns=cols)\n",
    "\n",
    "    index = len(columns) * len(lags)\n",
    "\n",
    "    for col in tqdm(columns, desc=\"Generating lag features\", leave=True):\n",
    "        for lag in tqdm(lags, desc=f\"Lag features for {col}\", leave=True):\n",
    "            lagged_column = f'{index}'\n",
    "            lag_df[lagged_column] = input_df[col].shift(lag)\n",
    "            index -= 1\n",
    "            \n",
    "    return lag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lag features for HR_Mean: 100%|██████████| 10/10 [00:00<00:00, 2493.79it/s]\n",
      "Lag features for TEMP_Mean: 100%|██████████| 10/10 [00:00<00:00, 2499.73it/s]\n",
      "Lag features for EDA_Mean: 100%|██████████| 10/10 [00:00<00:00, 2000.05it/s]\n",
      "Generating lag features: 100%|██████████| 3/3 [00:00<00:00, 132.28it/s]\n"
     ]
    }
   ],
   "source": [
    "cols = ['HR_Mean', 'TEMP_Mean', 'EDA_Mean']\n",
    "lags = [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
    "\n",
    "df_lag_features = generate_lag_features(df_features, cols, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9726, 30)\n"
     ]
    }
   ],
   "source": [
    "print(df_lag_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_lag_features, df_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9726, 49)\n"
     ]
    }
   ],
   "source": [
    "print(df_total.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Label Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(df_total, feature_columns):\n",
    "    scaled_df = df_total.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_df[feature_columns] = scaler.fit_transform(scaled_df[feature_columns])\n",
    "    scaled_df = scaled_df.fillna(0)\n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_label_regression(df_total, label_column, target_range=(0, 10)):\n",
    "    label_to_scale = df_total[label_column].values.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=target_range)\n",
    "    scaled_label = scaler.fit_transform(label_to_scale)\n",
    "    df_total[label_column] = scaled_label.flatten()\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_label_classifier(df_total, label_column):\n",
    "    df_total[label_column] = df_total[label_column].apply(lambda x: 0 if x <= 0.325 else (1 if 0.325 < x <= 0.65 else 2))\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_total.columns[:48]\n",
    "df_total_scaled_r = scale_features(df_total, feature_cols)\n",
    "df_total_scaled_r = scale_label_regression(df_total_scaled_r, 'STRESS', target_range=(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_total.columns[:48]\n",
    "df_total_scaled_c = scale_features(df_total, feature_cols)\n",
    "df_total_scaled_c = scale_label_classifier(df_total_scaled_c, 'STRESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final CSV File for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_scaled_r.to_csv(f'{processed_dataset_path}scaled_df_total_regression.csv', index=False)\n",
    "df_total_scaled_c.to_csv(f'{processed_dataset_path}scaled_df_total_classifier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9726, 49) (9726, 49)\n"
     ]
    }
   ],
   "source": [
    "print(df_total_scaled_r.shape, df_total_scaled_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{processed_dataset_path}scaled_df_total_regression.csv')\n",
    "\n",
    "X_r = data.iloc[:,0:48] # features\n",
    "Y_r = data.iloc[:,48:49] # labels\n",
    "\n",
    "X_train_r, X_val_r, Y_train_r, Y_val_r = train_test_split(X_r, Y_r, test_size=0.33, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=15)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=15)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=15)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100, max_depth=15)\n",
    "regressor.fit(X_train_r, Y_train_r.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{processed_dataset_path}scaled_df_total_classifier.csv')\n",
    "\n",
    "X_c = data.iloc[:,0:48] # features\n",
    "Y_c = data.iloc[:,48:49] # labels\n",
    "\n",
    "X_train_c, X_val_c, Y_train_c, Y_val_c = train_test_split(X_c, Y_c, test_size=0.33, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=15)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=15)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=15)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=15)\n",
    "clf.fit(X_train_c, Y_train_c.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = '%Y_%m_%d'  # Format for extracting only the date\n",
    "current_date_time_dt = dt.datetime.now()  # Current Date and Time in a DateTime Object.\n",
    "current_date_string = dt.datetime.strftime(current_date_time_dt, date_format) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Models/Regressor_Date_Time_2024_01_13.pkl']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naming system for model\n",
    "model_file_name_r = f'Regressor_Date_Time_{current_date_string}.pkl'\n",
    "model_save_path_r = model_path + model_file_name_r\n",
    "\n",
    "# saving the model\n",
    "joblib.dump(regressor, model_save_path_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Models/Classifier_Date_Time_2024_01_13.pkl']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naming system for model\n",
    "model_file_name_c = f'Classifier_Date_Time_{current_date_string}.pkl'\n",
    "model_save_path_c = model_path + model_file_name_c\n",
    "\n",
    "# saving the model\n",
    "joblib.dump(regressor, model_save_path_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_r = regressor.predict(X_val_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-like Metric: 0.394392523364486\n"
     ]
    }
   ],
   "source": [
    "# error tolerance level\n",
    "threshold = 0.5  \n",
    "\n",
    "correct_predictions = np.sum(np.abs(Y_val_r.values.ravel() - Y_pred_r) <= threshold)\n",
    "accuracy_like_metric = correct_predictions / len(Y_val_r)\n",
    "\n",
    "print('Accuracy-like Metric:', accuracy_like_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_c = clf.predict(X_val_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.938006230529595\n",
      "pre = 0.9348169485254413\n",
      "recall = 0.9218761497506739\n",
      "f1 = 0.9274954029824208\n"
     ]
    }
   ],
   "source": [
    "f1score   = f1_score        (Y_val_c, Y_pred_c, average = 'macro')\n",
    "recall    = recall_score    (Y_val_c, Y_pred_c, average = 'macro')\n",
    "precision = precision_score (Y_val_c, Y_pred_c, average = 'macro')\n",
    "accuracy  = accuracy_score  (Y_val_c, Y_pred_c)\n",
    "\n",
    "print('acc =', accuracy)\n",
    "print('pre =', precision)\n",
    "print('recall =', recall) \n",
    "print('f1 =', f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
